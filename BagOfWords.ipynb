{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus = pd.read_pickle('./Dados/preprocessed_corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(df_corpus['sample'])"
   ]
  },
  {
   "source": [
    "# Bag of Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### CountVectorizer: \n",
    "\n",
    "Cria uma matriz esparsa, em que cada coluna representa uma palavra única do vocabulário e cada linha representa um documento. Os valores preenchidos nas colunas representam a contagem de ocorrências daquela palavra (nome da coluna) naquele documento (linha).\n",
    "\n",
    "No caso do nosso dataset, temos:\n",
    "\n",
    "* 7200 linhas, uma para cada amostra (notícia) \n",
    "    * 2600 são falsas\n",
    "    * 2600 são verdadeiras\n",
    "\n",
    "Parâmetros:\n",
    "\n",
    "* __lowercase__: converte todos os caracteres para minúsculo antes de tokenizar (default)\n",
    "    \n",
    "* __tokenizer__: define de que forma as features serão tokenizadas\n",
    "    * _None_: será tokenizada a partir das palavras (default)\n",
    "\n",
    "* __preprocessor__: define quais serão os pré-processamentos aplicados ao dataset\n",
    "    * _None_: torna todas as letras minúsculas e retira os acentos (default)\n",
    "\n",
    "* __stop_words__: define quais palavras serão removidas das features\n",
    "    * _None_: nenhuma palavra é removida (default)\n",
    "\n",
    "* __token_pattern__: define o que constitui um token\n",
    "    * _r'(?u)\\b\\w\\w+\\b'_: 2 ou mais caracteres alfanuméricos - pontuação é completamente ignorada (default)\n",
    "\n",
    "* __analyzer__: define a unidade de granulação das features\n",
    "    * _word_: cada token vai conter uma palavra\n",
    "    * _char_: cada token vai conter um caractere\n",
    "\n",
    "* __ngram_range__: define quantos itens seguidos vão constituir um token. \n",
    "    * _(1,1)_: cria apenas unigramas (default)\n",
    "    * _(1,2)_: cria unigramas e bigramas\n",
    "\n",
    ">'esse é um exemplo'\n",
    "\n",
    ">analyzer = word; ngram = 1;\n",
    "\n",
    ">\\['esse','é','um','exemplo']\n",
    "\n",
    ">analyzer = word; ngram = 2;\n",
    "\n",
    ">\\['esse é','um exemplo']\n",
    "\n",
    ">analyzer = word; ngram_range = (1,2)\n",
    "\n",
    ">\\['esse','é','um','exemplo','esse é','um exemplo']\n",
    "\n",
    "* __max_df__: define um limite máximo de vezes em que um token pode aparecer antes de ser desconsiderado - detecta stop words\n",
    "    * _1.0_ (float): 100%, ou seja, nenhum token é desconsiderado (default)\n",
    "    * _1_ (int): desconsidera todos os tokens que aparecerem mais de uma vez\n",
    "\n",
    "* __min_df__: define um limite mínimo de vezes em que um token pode aparecer antes de ser desconsiderado - detecta cut-off words\n",
    "    * _1_ (int): desconsidera todos os tokens que aparecem menos de uma vez (default)\n",
    "    * _1.0_ (float): 100%, ou seja, desconsidera todos os tokens\n",
    "\n",
    "* __max_features__: define número máximo de features do vetor ordenado pela frequência das palavras\n",
    "    * None: todas as palavras que aparecem no corpus serão consideradas (default)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase = True,\n",
    "                             analyzer = 'word',\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_BoW = vectorizer.fit_transform(corpus)\n",
    "X_BoW = X_BoW.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7200, 83032)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "X_BoW.shape"
   ]
  },
  {
   "source": [
    "# Bag of Words com Stopwords"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ELOGROUP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = stopwords,\n",
    "                             max_features = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_BoW_stopwords = vectorizer.fit_transform(corpus)\n",
    "X_BoW_stopwords = X_BoW_stopwords.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Bow_stopwords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"./Dados/X_BoW_stopwords.csv\", X_BoW_stopwords, delimiter=\",\")"
   ]
  },
  {
   "source": [
    "# Bag of Words com Stopwords e Stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\ELOGROUP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping stemmers\\rslp.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('rslp')\n",
    "stemmer = nltk.stem.RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = CountVectorizer().build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=stemmed_words,\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = stopwords,\n",
    "                             max_features = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_BoW_stem = vectorizer.fit_transform(corpus)\n",
    "X_BoW_stem = X_BoW_stem.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Bow_stem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"./Dados/X_BoW_stem.csv\", X_BoW_stem, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}